{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INF8111 - Fouille de données\n",
    "\n",
    "\n",
    "## TP2 Automne 2019 - Détection de discours d'incitation à la haine\n",
    "\n",
    "##### Membres de l'équipe:\n",
    "\n",
    "    - MAINKA Adrien (2046374)\n",
    "    - BAKKAOUI Mehdi (2038803)\n",
    "    - METAIS Marianne (2038757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import words_repeated_char\n",
    "import sys\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"universal_tagset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path, delimiter = ' '):\n",
    "    \"\"\" We had some issues loading the data using pandas.read_csv, so we built our own loader.\n",
    "        Read a csv file, returns a pandas.Dataframe\n",
    "    \"\"\"\n",
    "    fp = open(file_path, encoding=\"utf8\")\n",
    "    line = fp.readline()\n",
    "    data_dict = dict()\n",
    "    labels = line[:-1].split(delimiter)\n",
    "    line = fp.readline()\n",
    "    for label in labels:\n",
    "        data_dict[label] = []\n",
    "    while line:\n",
    "        for i, j in enumerate(line[:-1].split(delimiter)):\n",
    "            data_dict[labels[i]].append(j)\n",
    "        line = fp.readline()\n",
    "    return pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(\"data/hateful_tweets_filtered.csv\", delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7629505483</td>\n",
       "      <td>abusive</td>\n",
       "      <td>I fuckin hate when niggas stare at me fuck r u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12181574836</td>\n",
       "      <td>abusive</td>\n",
       "      <td>everyone jumps to silly conclusions as soon as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25381445793</td>\n",
       "      <td>abusive</td>\n",
       "      <td>#sincewerebeinghonest I'm Emo. I Need A Fuckin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192730077165916160</td>\n",
       "      <td>abusive</td>\n",
       "      <td>banana bread recipe: 1. get some bread 2. i do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>280882735374028800</td>\n",
       "      <td>abusive</td>\n",
       "      <td>godamn this bitch so bad i want 2 drink molly ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id    label  \\\n",
       "0          7629505483  abusive   \n",
       "1         12181574836  abusive   \n",
       "2         25381445793  abusive   \n",
       "3  192730077165916160  abusive   \n",
       "4  280882735374028800  abusive   \n",
       "\n",
       "                                       tweet_content  \n",
       "0  I fuckin hate when niggas stare at me fuck r u...  \n",
       "1  everyone jumps to silly conclusions as soon as...  \n",
       "2  #sincewerebeinghonest I'm Emo. I Need A Fuckin...  \n",
       "3  banana bread recipe: 1. get some bread 2. i do...  \n",
       "4  godamn this bitch so bad i want 2 drink molly ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessed = []\n",
    "for i, tweet in enumerate(data[\"tweet_content\"].values[:]):\n",
    "    tmp = tweet.lower().replace(\"\\\\n\",' ').replace('\\\\xa0', ' ').replace('\\\\r', ' ').replace(\"\\\\'\",\"'\").replace(\"&lt;\",'<')\\\n",
    "         .replace(\"&gt;\",'>').replace(\" &amp; \", \" and \").replace(\"&amp;\", \"&\")\n",
    "    # lower the tweet and replacing characters that tweeter has tranlated to their hmtl numeric code to their original value\n",
    "    tmp = re.sub(\"(http(s)?://)?(www\\.)?([a-zA-Z0-9])+\\.[a-z]{1,3}(/\\S*)?\",'URL', tmp) # \n",
    "    tmp = re.sub(\"#\\w+\", 'HASHTAG', tmp)\n",
    "    tmp = re.sub(\"@\\w+\", 'USER', tmp)\n",
    "    tmp = re.sub(\"\\w+@\\w+\\.[a-z]{2,3}\", \"EMAIL\", tmp)\n",
    "    tmp = re.sub(\"[0-9]{1,2}/[0-9]{1,2}/([0-9]{4}|[0-9]{2})|([0-9]{4}|[0-9]{2})/[0-9]{1,2}/[0-9]{1,2}|[0-9]{2}/[0-9]{2}\", \"DATE\", tmp)\n",
    "    tmp = re.sub(\"[0-9]{2}(:[0-9]{2}){1,2}( ?(am|pm))?\", \"TIME\", tmp)\n",
    "    tmp = re.sub(\"(([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF]))+\", ' EMOJI ', tmp)\n",
    "    tmp = tmp.encode(encoding='ascii', errors='ignore').decode()\n",
    "    # get rid of the non ascii characters\n",
    "    tmp = re.sub(\"\\.{2,}\", \"...\", tmp)\n",
    "    for c in ['!', '-', ',']:\n",
    "        tmp = re.sub(\"{}+\".format(c), c, tmp)\n",
    "    tmp = re.sub(\"\\?+\", \"?\", tmp)\n",
    "    tmp = re.sub(\"`+\", \"'\", tmp)\n",
    "    tmp = re.sub(\"'{2,}\", \"'\", tmp)\n",
    "    tmp = tmp.translate(str.maketrans(dict.fromkeys('#*+/<=>@[\\\\]^_`{|}~'))) #removing all the other special characters\n",
    "    tokens = [t if t not in [\"''\", \"``\"] else '\"' for t in word_tokenize(tmp) ]\n",
    "    # tokenizing using nltk.word_tokenize. althought it transforms '\"' into '``' or \"''\" and this is a behaviour do not want\n",
    "    # so we make sure that the '\"' are changed to their original form\n",
    "    for i, token in enumerate(tokens):\n",
    "        if re.search(r\"([a-z])\\1{2,}\", token):\n",
    "            # cleaning the words containing a letter repeated 3 times or more, using the list of the Ensglish words\n",
    "            tokens[i] = words_repeated_char.clean(token)\n",
    "    preprocessed.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"preprocessed\"] = preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet_content</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7629505483</td>\n",
       "      <td>abusive</td>\n",
       "      <td>I fuckin hate when niggas stare at me fuck r u...</td>\n",
       "      <td>[i, fuckin, hate, when, niggas, stare, at, me,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12181574836</td>\n",
       "      <td>abusive</td>\n",
       "      <td>everyone jumps to silly conclusions as soon as...</td>\n",
       "      <td>[everyone, jumps, to, silly, conclusions, as, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25381445793</td>\n",
       "      <td>abusive</td>\n",
       "      <td>#sincewerebeinghonest I'm Emo. I Need A Fuckin...</td>\n",
       "      <td>[HASHTAG, i, 'm, emo, ., i, need, a, fucking, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192730077165916160</td>\n",
       "      <td>abusive</td>\n",
       "      <td>banana bread recipe: 1. get some bread 2. i do...</td>\n",
       "      <td>[banana, bread, recipe, :, 1., get, some, brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>280882735374028800</td>\n",
       "      <td>abusive</td>\n",
       "      <td>godamn this bitch so bad i want 2 drink molly ...</td>\n",
       "      <td>[godamn, this, bitch, so, bad, i, want, 2, dri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id    label  \\\n",
       "0          7629505483  abusive   \n",
       "1         12181574836  abusive   \n",
       "2         25381445793  abusive   \n",
       "3  192730077165916160  abusive   \n",
       "4  280882735374028800  abusive   \n",
       "\n",
       "                                       tweet_content  \\\n",
       "0  I fuckin hate when niggas stare at me fuck r u...   \n",
       "1  everyone jumps to silly conclusions as soon as...   \n",
       "2  #sincewerebeinghonest I'm Emo. I Need A Fuckin...   \n",
       "3  banana bread recipe: 1. get some bread 2. i do...   \n",
       "4  godamn this bitch so bad i want 2 drink molly ...   \n",
       "\n",
       "                                        preprocessed  \n",
       "0  [i, fuckin, hate, when, niggas, stare, at, me,...  \n",
       "1  [everyone, jumps, to, silly, conclusions, as, ...  \n",
       "2  [HASHTAG, i, 'm, emo, ., i, need, a, fucking, ...  \n",
       "3  [banana, bread, recipe, :, 1., get, some, brea...  \n",
       "4  [godamn, this, bitch, so, bad, i, want, 2, dri...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set :  42056\n",
      "Length of validation set :  9233\n",
      "Length of test set :  9052\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = preprocessed\n",
    "y = []\n",
    "\n",
    "\n",
    "for i in range(len(data)):\n",
    "    \n",
    "    if data[\"label\"][i] == \"hateful\":\n",
    "        y.append(0)\n",
    "    elif data[\"label\"][i] == \"abusive\":\n",
    "        y.append(1)\n",
    "    elif data[\"label\"][i] == \"spam\":\n",
    "        y.append(2)\n",
    "    elif data[\"label\"][i] == \"normal\":\n",
    "        y.append(3)\n",
    "\n",
    "train_valid_X, test_X, train_valid_Y, test_Y = train_test_split(X, y, test_size=0.15, random_state=12)\n",
    "\n",
    "train_X, valid_X, train_Y, valid_Y = train_test_split(train_valid_X, train_valid_Y, test_size=0.18, random_state=12)\n",
    "\n",
    "print(\"Length of training set : \", len(train_X))\n",
    "print(\"Length of validation set : \", len(valid_X))\n",
    "print(\"Length of test set : \", len(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "class Stemmer(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "    def stem(self, tweet):\n",
    "        \"\"\"\n",
    "        tweet: a list of string\n",
    "        \"\"\"\n",
    "        # Have to return the stemmed tweet\n",
    "        return list(map(self.stemmer.stem , tweet))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=Stemmer()\n",
    "X = list(map(s.stem, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    # This function returns the list of bigrams\n",
    "    if len(tokens) < 2:\n",
    "        return []\n",
    "    bigrams = []\n",
    "    previous_word = tokens[0]\n",
    "    for word in tokens[1:]:\n",
    "        bigrams.append(previous_word + \" \" + word)\n",
    "        previous_word = word\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def trigram(tokens):\n",
    "    \"\"\"\n",
    "    tokens: a list of strings\n",
    "    \"\"\"\n",
    "    # This function returns the list of trigrams\n",
    "    if len(tokens) < 3:\n",
    "        return []\n",
    "    trigrams = []\n",
    "    previous_word = tokens[1]\n",
    "    previous_previous_word = tokens[0]\n",
    "    for word in tokens[2:]:\n",
    "        trigrams.append(previous_previous_word + \" \" + previous_word + \" \" + word)\n",
    "        previous_previous_word = previous_word\n",
    "        previous_word = word\n",
    "    return trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "import numpy as np\n",
    "\n",
    "class CountBoW(object):\n",
    "\n",
    "    def __init__(self, bigram=False, trigram=False):\n",
    "        \"\"\"\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        words: list of words in the vocabulary\n",
    "        \"\"\"\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.words = None\n",
    "        \n",
    "\n",
    "    def computeBoW(self, tokens):\n",
    "        \"\"\"\n",
    "        Calcule du BoW, à partir d'un dictionnaire de mots et d'une liste de tweets.\n",
    "        On suppose que l'on a déjà collecté le dictionnaire sur l'ensemble d'entraînement.\n",
    "        \n",
    "        Entrée: tokens, une liste de vecteurs contenant les tweets (une liste de liste)\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "        \n",
    "        bow = lil_matrix((len(tokens), len(self.words)))\n",
    "\n",
    "        for i, tweet in enumerate(tokens):\n",
    "            for token in tweet:\n",
    "                if token in self.words:\n",
    "                    bow[i, self.words.index(token)] += 1\n",
    "        \n",
    "        BoW = csr_matrix(bow)\n",
    "        return BoW\n",
    "                \n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
    "        si besoin, et transforme les textes en vecteurs d'entiers.\n",
    "        \n",
    "        Entrée: X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        tokens = X\n",
    "        \n",
    "        tokens_bi_tri = list(tokens)\n",
    "            \n",
    "        if self.bigram:\n",
    "            for i, tweet in enumerate(tokens):\n",
    "                tokens_bi_tri[i] = tokens_bi_tri[i] + bigram(tweet)\n",
    "        \n",
    "        if self.trigram:\n",
    "            for i, tweet in enumerate(tokens):\n",
    "                tokens_bi_tri[i] = tokens_bi_tri[i] + trigram(tweet)\n",
    "                \n",
    "        words = []\n",
    "        for tweet in tokens_bi_tri:\n",
    "            for token in tweet:\n",
    "                if token not in words:\n",
    "                    words.append(token)\n",
    "\n",
    "        self.words = words\n",
    "        return self.computeBoW(tokens_bi_tri)\n",
    "        \n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
    "        si besoin, et transforme les textes en vecteurs d'entiers.\n",
    "        Différence avec fit_transform : on suppose qu'on dispose déjà du dictionnaire ici\n",
    "\n",
    "        Entrée: X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "\n",
    "        tokens = X\n",
    "        tokens_bi_tri = list(tokens)\n",
    "        if self.bigram:\n",
    "            for i, tweet in enumerate(tokens):\n",
    "                tokens_bi_tri[i] = tokens_bi_tri[i] + bigram(tweet)\n",
    "        \n",
    "        if self.trigram:\n",
    "            print(\"bow tfidf\")\n",
    "            for i, tweet in enumerate(tokens):\n",
    "                tokens_bi_tri[i] = tokens_bi_tri[i] + trigram(tweet)\n",
    "        \n",
    "        return self.computeBoW(tokens_bi_tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "import math\n",
    "\n",
    "\n",
    "class TFIDFBoW(object):\n",
    "\n",
    "    def __init__(self, bigram=False, trigram=False):\n",
    "        \"\"\"\n",
    "        bigram: enable or disable bigram\n",
    "        trigram: enable or disable trigram\n",
    "        words: list of words in the vocabulary\n",
    "        idf: list of idfs for each document\n",
    "        \"\"\"\n",
    "        self.bigram = bigram\n",
    "        self.trigram = trigram\n",
    "        self.words = None\n",
    "        self.idf = None\n",
    "        \n",
    "        self.cBoW = None\n",
    "    \n",
    "    def computeTFIDF(self, X):\n",
    "        \"\"\"\n",
    "        Calcule du TF-IDF, à partir d'un dictionnaire de mots et d'une \n",
    "        liste de tweets.\n",
    "        On suppose que l'on a déjà collecté le dictionnaire ainsi que \n",
    "        calculé le vecteur contenant l'idf pour chaque document.\n",
    "        \n",
    "        Entrée : X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        print(\"computeTFIDF...\")\n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "        \n",
    "        tf = self.cBoW.computeBoW(X)\n",
    "                \n",
    "        for j in range(len(X)):\n",
    "            tf[j] = tf[j].multiply(self.idf)\n",
    "                \n",
    "        TFIDF = csr_matrix(tf)\n",
    "        return TFIDF\n",
    "\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
    "        si besoin, et transforme les textes en vecteurs de flottants avec la pondération TF-IDF.\n",
    "        \n",
    "        Entrée : X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        print(\"fit_transform...\")\n",
    "        \n",
    "        tokens = X\n",
    "        \n",
    "        print(\"compute bow...\")\n",
    "        \n",
    "        cBoW = CountBoW(bigram = self.bigram, trigram = self.trigram)\n",
    "        bow = cBoW.fit_transform(X)\n",
    "        self.cBoW = cBoW\n",
    "        \n",
    "        self.words = cBoW.words\n",
    "\n",
    "        print(\"compute idf...\")\n",
    "        df = np.diff(bow.tocsc().indptr)\n",
    "        df = (list(map(lambda x: math.log(len(X)/x), df)))\n",
    "        self.idf = df\n",
    "        \n",
    "        tokens_bi_tri = list(tokens)\n",
    "        if self.bigram:\n",
    "            for i, tweet in enumerate(tokens):\n",
    "                tokens_bi_tri[i] = tokens_bi_tri[i] + bigram(tweet)\n",
    "        \n",
    "        if self.trigram:\n",
    "            for i, tweet in enumerate(tokens):\n",
    "                tokens_bi_tri[i] = tokens_bi_tri[i] + trigram(tweet)\n",
    "        \n",
    "        return self.computeTFIDF(tokens_bi_tri)\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Cette méthode preprocess les données en utilisant la pipeline, ajoute les bigram et trigram \n",
    "        si besoin, et transforme les textes en vecteurs de flottants avec la pondération TF-IDF.\n",
    "        Différence avec fit_transform : on suppose qu'on dispose déjà du dictionnaire et du calcul des idf ici.\n",
    "            \n",
    "        Entrée : X, une liste de vecteurs contenant les tweets\n",
    "        \n",
    "        Return: une csr_matrix\n",
    "        \"\"\"\n",
    "        print(\"transform\")\n",
    "\n",
    "        if self.words is None:\n",
    "            raise Exception(\n",
    "                \"fit_transform() should be called first (no dictionnary available)\"\n",
    "            )\n",
    "\n",
    "        tokens = X\n",
    "        tokens_bi_tri = list(tokens)\n",
    "        if self.bigram:\n",
    "            for i, tweet in enumerate(tokens):\n",
    "                tokens_bi_tri[i] = tokens_bi_tri[i] + bigram(tweet)\n",
    "        \n",
    "        if self.trigram:\n",
    "            for i, tweet in enumerate(tokens):\n",
    "                tokens_bi_tri[i] = tokens_bi_tri[i] + trigram(tweet)\n",
    "        \n",
    "        return self.computeTFIDF(tokens_bi_tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train_evaluate(classifier, training_X, training_Y, validation_X, validation_Y, bowObj):\n",
    "    \"\"\"\n",
    "    classifier: model used (LogisticRegression...)\n",
    "    training_X: tweets from the training dataset\n",
    "    training_Y: tweet labels from the training dataset\n",
    "    validation_X: tweets from the validation dataset\n",
    "    validation_Y: tweet labels from the validation dataset\n",
    "    bowObj: Bag-of-word object\n",
    "    \n",
    "    :return: the classifier and its accuracy in the training and validation dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    training_rep = bowObj.fit_transform(training_X)\n",
    "\n",
    "    classifier.fit(training_rep, training_Y)\n",
    "\n",
    "    trainAcc = accuracy_score(training_Y, classifier.predict(training_rep))\n",
    "    validationAcc = accuracy_score(\n",
    "        validation_Y, classifier.predict(bowObj.transform(validation_X)))\n",
    "\n",
    "    return classifier, trainAcc, validationAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_transform...\n",
      "compute bow...\n",
      "compute idf...\n",
      "computeTFIDF...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mehdi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\mehdi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\mehdi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:1297: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n",
      "  \" = {}.\".format(effective_n_jobs(self.n_jobs)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transform\n",
      "computeTFIDF...\n"
     ]
    }
   ],
   "source": [
    "trainAcc = []\n",
    "validationAcc = []\n",
    "\n",
    "# LogisticRegression + TFIDFBoW + unigram\n",
    "tb = TFIDFBoW(bigram = False,trigram = False)\n",
    "model=train_evaluate(LogisticRegression(n_jobs=-1),test_X, test_Y, valid_X, valid_Y, tb)\n",
    "trainAcc.append(model[1])\n",
    "validationAcc.append(model[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "r=np.arange(len(trainAcc))\n",
    "\n",
    "TypeAcc = [\"Training\", \"Validation\"]\n",
    "bar_width = 0.35\n",
    "\n",
    "plt.bar(r, trainAcc,bar_width)\n",
    "plt.bar(r+bar_width, validationAcc,bar_width)\n",
    "plt.xticks(r+bar_width/2, ['LogisticRegression', '2', '3', '4', '5', '6'])\n",
    "\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.title('Models comparison')\n",
    "\n",
    "plt.legend(TypeAcc)\n",
    "\n",
    "for i in range(len(trainAcc)):\n",
    "    plt.text(x = r[i] , y = trainAcc[i]+0.01, s = '%.4f' % trainAcc[i])\n",
    "    plt.text(x = r[i]+bar_width , y = validationAcc[i]+0.01, s = '%.4f' % validationAcc[i])\n",
    "\n",
    "plt.ylim(.5, 1.2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
