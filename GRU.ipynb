{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "RNN.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XBsfQDpc9Un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re, torch, time\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U55XbJOXc9VB",
        "colab_type": "code",
        "outputId": "893e335f-2807-4448-de92-901e856c7c39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eTVZFV8deDE",
        "colab_type": "code",
        "outputId": "4a6b5449-37f0-495d-d410-4197f4a8f80c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgcKn7dwc9VX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_pickle(\"/content/drive/My Drive/Colab Notebooks/INF8111/data/preprocess.pkl\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCn_iC3cc9Vr",
        "colab_type": "code",
        "outputId": "b93526c5-e88a-47d5-8c70-62798f4d0cfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "plt.hist([len(x) for x in data.tweet], bins=50)\n",
        "plt.title(\"Sequence length distribution\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Sequence length distribution')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEICAYAAABF82P+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkroMCElSlwEhSeoyICRJXf8f9oeTDT+EhLAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvHYOllzz73B",
        "colab_type": "code",
        "outputId": "edf5b8b8-bed6-4d5d-f71f-359bbac7def0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "seq_len = 30\n",
        "print(\"Mean sequence length: {:.2f}\".format(np.array([len(x) for x in data.tweet]).mean()))\n",
        "print(\"{:.2f}% of the samples are shorter than {}\".format(100*len([len(x) for x in data.tweet if len(x)<=seq_len])/len(data), seq_len))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean sequence length: 19.07\n",
            "95.09% of the samples are shorter than 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgJY4N4Ic9V2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text2seq(X, y, vocab_max, seq_len, unk_tok='UNK', padd_tok='PAD' ):\n",
        "  \"\"\" For a list of inputs X with different sequence length, associated to the labels y.\n",
        "      Builds the list of the vocab_max-2 most common words. Maps each of them to a number.\n",
        "      Replace the other words by the token unk_tok, which is mapped to the value 1.\n",
        "      Then force the input to have a sequence length of seq_len with this policy:\n",
        "        - If the sample x is short than seq_len, then adds padding at the begining.\n",
        "          The padding token is set by the variable padd_tok. And mapped to the value 0.\n",
        "        - If the sample x is longer than seq_len, then extracts (len(x)//seq_len)+1\n",
        "          sequences from x.\n",
        "  \"\"\"\n",
        "  count_words = Counter()\n",
        "  for post in X:\n",
        "    for tok in post:\n",
        "      count_words[tok] += 1\n",
        "  vocab = {w for w,_ in count_words.most_common(vocab_max-2)}.union( {unk_tok, padd_tok})\n",
        "  word2val = {w : i+2 for i, (w,_) in enumerate( count_words.most_common(vocab_max-2) )}\n",
        "  word2val[padd_tok] = 0\n",
        "  word2val[unk_tok] = 1\n",
        "\n",
        "  X2 = []\n",
        "  y2 = []\n",
        "  for i, x in enumerate(X):\n",
        "    if len(x) < seq_len:\n",
        "      tmp = x.copy()\n",
        "      for _ in range(seq_len - len(x)):\n",
        "        tmp.insert(0, padd_tok)\n",
        "      X2.append([word2val[tok] if tok in vocab else 1 for tok in tmp])\n",
        "      y2.append(y[i])\n",
        "    elif len(x) == seq_len:\n",
        "      X2.append([word2val[tok] if tok in vocab else 1 for tok in x]) \n",
        "      y2.append(y[i])\n",
        "    else :\n",
        "      for j in range(len(x)//seq_len):\n",
        "        X2.append([word2val[tok] if tok in vocab else 1 for tok in  x[j*seq_len:(j+1)*seq_len]])\n",
        "        y2.append(y[i])\n",
        "      X2.append([word2val[tok] if tok in vocab else 1 for tok in  x[-seq_len:]])\n",
        "      y2.append(y[i])\n",
        "  return word2val, X2, y2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaevReF6lMD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = Counter()\n",
        "for tweet in data.tweet.values:\n",
        "  for tok in tweet:\n",
        "    count[tok] += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ68QPm2lt_T",
        "colab_type": "code",
        "outputId": "da986522-45ff-4eb6-c27f-b93bea0d8f8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "seq_len = 30\n",
        "hapax = [a for a in count if count[a] == 1]\n",
        "print(\"Vocab_size: {}\\t Hapax: {}\\tVocab_withouthapax: {}\".format(len(count), len(hapax), len(count) - len(hapax)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab_size: 63465\t Hapax: 37849\tVocab_withouthapax: 25616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ay7EXKwDlJAX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_max = len(count) - len(hapax)\n",
        "word2val, X, y = text2seq(data.tweet.values, data.label.values, vocab_max, seq_len) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQFW2Y4vc9WF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.5, random_state=42)\n",
        "X_train = torch.tensor(X_train)\n",
        "X_valid = torch.tensor(X_valid)\n",
        "X_test = torch.tensor(X_test)\n",
        "y_train = torch.tensor(y_train)\n",
        "y_valid = torch.tensor(y_valid)\n",
        "y_test = torch.tensor(y_test)\n",
        "train_data = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "valid_data = torch.utils.data.TensorDataset(X_valid, y_valid)\n",
        "test_data = torch.utils.data.TensorDataset(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP_e5_Gqnrzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UV-k6sFc9WP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size,\n",
        "                 hidden_size,\n",
        "                 vocab_size,\n",
        "                 num_layers,\n",
        "                 output_size,\n",
        "                 nonlinearity = 'relu',\n",
        "                 bias = True,\n",
        "                 dropout = 0,\n",
        "                 bidirectional = True\n",
        "                ):\n",
        "        super(GRU, self).__init__()\n",
        "        #hyper-parameters\n",
        "        self.emb_size      = emb_size\n",
        "        self.hidden_size   = hidden_size\n",
        "        self.vocab_size    = vocab_size\n",
        "        self.num_layers    = num_layers\n",
        "        self.output_size   = output_size\n",
        "        self.nonlinearity  = nonlinearity\n",
        "        self.bias          = bias\n",
        "        self.dropout       = dropout\n",
        "        self.bidirectional = bidirectional\n",
        "        #layers\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_size).to(device)\n",
        "        self.gru = nn.GRU(input_size = emb_size,\n",
        "                          hidden_size = hidden_size,\n",
        "                          num_layers = num_layers,\n",
        "                          bias = bias,\n",
        "                          dropout = dropout,\n",
        "                          bidirectional = bidirectional\n",
        "                         ).to(device)\n",
        "        self.linear = nn.Linear((bidirectional+1) *hidden_size, output_size).to(device)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        out = self.embedding(X.t())\n",
        "        out, _ = self.gru(out)\n",
        "        return self.linear(out[-1])\n",
        "    \n",
        "    def predict(self, data):\n",
        "        if isinstance(data, torch.utils.data.DataLoader):\n",
        "            y = []\n",
        "            for i in range((len(data.dataset)//data.batch_size)+1):\n",
        "                imin = i*data.batch_size\n",
        "                imax = min((i+1)*batch_size, len(data.dataset))\n",
        "                y.append(self.forward(data.dataset[imin:imax][0].to(device)).max(dim=1)[1])\n",
        "            return torch.cat(y)\n",
        "        elif isinstance(data, torch.Tensor):\n",
        "            return self.forward(data.to(device)).max(dim=1)[1]\n",
        "        elif isinstance(data, torch.utils.data.dataset.TensorDataset):\n",
        "            return self.predict(data.tensors[0])\n",
        "        else:\n",
        "            raise AttributeError(\"data must be an instance of DataLoader, Tensor or TensorDataset\")\n",
        "        \n",
        "\n",
        "    def accuracy(self, X, y=None):\n",
        "        if isinstance(X, torch.utils.data.DataLoader):\n",
        "            return (self.predict(X) == X.dataset.tensors[1].to(device)).float().sum().item() / len(X.dataset.tensors[1])\n",
        "        elif isinstance(X, torch.Tensor) and isinstance(y, torch.Tensor):\n",
        "            return (self.predict(X.to(device)) == y.to(device)).float().sum().item() / len(y)\n",
        "        else :\n",
        "            raise AttributeError(\"Must be called with X as a DataLoader or X and y some Tensor\")\n",
        "    \n",
        "    def f1_score(self, data_loader, label=1):\n",
        "        return f1_score((data_loader.dataset[:][1] == label).int().cpu().numpy(),\n",
        "                        (self.predict(data_loader) == label).int().cpu().numpy() )\n",
        "\n",
        "    def run_epoch(self, n_epochs, criterion, optimizer, train_loader,\n",
        "                  valid_loader=None, early_stop=True, patience=1 ):\n",
        "        start_time = time.time()\n",
        "        last_time = time.time()\n",
        "        if valid_loader:\n",
        "            best_score = -1\n",
        "            best_param = None\n",
        "            epochs_since_best_score = 0\n",
        "        for epoch in range(1, n_epochs+1):\n",
        "            print(\"Epoch: {}/{}\".format(epoch, n_epochs))\n",
        "            running_loss = 0\n",
        "            for i, (X_batch, y_batch) in enumerate(train_loader):\n",
        "                y_batch = y_batch.to(device)\n",
        "                torch.cuda.empty_cache()\n",
        "                optimizer.zero_grad()\n",
        "                out = self.forward(X_batch.to(device))\n",
        "                loss = criterion(out, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "            train_score = self.f1_score(train_loader, label=0)\n",
        "            if valid_loader:\n",
        "                valid_score = self.f1_score(valid_loader, label=0)\n",
        "                print(\"loss: {:.6f}\\ttrain_f1: {:.4f}\\telapsed_time: {:.1f}s\\tvalid_f1: {:.4f}\".format(\n",
        "                    running_loss/len(train_loader.dataset), train_score, time.time()-start_time, valid_score))\n",
        "                if best_score > valid_score:\n",
        "                    epochs_since_best_score +=1\n",
        "                    if early_stop and epochs_since_best_score == patience:\n",
        "                        print(\"Early stopping. Resetting the parameters of the\\\n",
        "                        best score on the validation set.\")\n",
        "                        self.load_state_dict(best_param)\n",
        "                        return\n",
        "                else:\n",
        "                    best_score  = valid_score\n",
        "                    best_param = copy.deepcopy(self.state_dict())\n",
        "                    epochs_since_best_score = 0\n",
        "            else:\n",
        "                print(\"loss: {:.6f}\\ttrain_f1: {:.4f}\\telapsed_time: {:.1f}s\".format(\n",
        "                    running_loss/len(train_loader.dataset), train_score, time.time()-start_time))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM20Y5YCc9WV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = 'GRU'\n",
        "emb_size = 16\n",
        "hidden_size = 128\n",
        "num_layers = 4\n",
        "output_size = 4\n",
        "bidirectional=True\n",
        "filename = '{}{}_emb{}_hid{}_lay{}_vocab{}.pt'.format('bi' if bidirectional else '', model, emb_size, hidden_size, num_layers, vocab_max )\n",
        "\n",
        "\n",
        "gru = GRU(emb_size = emb_size, hidden_size = hidden_size, vocab_size = vocab_max,\n",
        "          num_layers = num_layers, output_size = output_size, bidirectional=bidirectional)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6CnMqxdc9Wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weight = compute_class_weight('balanced', np.unique(y_train.unique()), y_train.numpy())\n",
        "criterion = nn.CrossEntropyLoss(weight=torch.Tensor(class_weight).to(device))\n",
        "optimizer = torch.optim.Adam(gru.parameters(), weight_decay=10e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FrTNiWlOcCs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "cf1c1df3-467b-458f-e83b-37d25717a2a4"
      },
      "source": [
        "gru.run_epoch(n_epochs=20, criterion=criterion, optimizer=optimizer, train_loader=train_loader, valid_loader=valid_loader, early_stop=True, patience=2)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20\n",
            "loss: 0.008111\ttrain_f1: 0.1364\telapsed_time: 11.8s\tvalid_f1: 0.1323\n",
            "Epoch: 2/20\n",
            "loss: 0.007002\ttrain_f1: 0.2112\telapsed_time: 23.7s\tvalid_f1: 0.1992\n",
            "Epoch: 3/20\n",
            "loss: 0.006313\ttrain_f1: 0.2097\telapsed_time: 35.5s\tvalid_f1: 0.2013\n",
            "Epoch: 4/20\n",
            "loss: 0.005821\ttrain_f1: 0.3041\telapsed_time: 47.4s\tvalid_f1: 0.2794\n",
            "Epoch: 5/20\n",
            "loss: 0.005523\ttrain_f1: 0.3250\telapsed_time: 59.2s\tvalid_f1: 0.2663\n",
            "Epoch: 6/20\n",
            "loss: 0.005229\ttrain_f1: 0.3494\telapsed_time: 71.1s\tvalid_f1: 0.2928\n",
            "Epoch: 7/20\n",
            "loss: 0.004938\ttrain_f1: 0.3958\telapsed_time: 82.9s\tvalid_f1: 0.3111\n",
            "Epoch: 8/20\n",
            "loss: 0.004642\ttrain_f1: 0.3570\telapsed_time: 94.7s\tvalid_f1: 0.2734\n",
            "Epoch: 9/20\n",
            "loss: 0.004347\ttrain_f1: 0.3865\telapsed_time: 106.7s\tvalid_f1: 0.2771\n",
            "Early stopping. Resetting the parameters of the                         best accuracy on the validation set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggeD4ahXzqjQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "b230e882-2b58-4704-e4ab-df8d28278423"
      },
      "source": [
        "print(classification_report(y_test.cpu().numpy(), gru.predict(test_loader).cpu().numpy()))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.57      0.33       654\n",
            "           1       0.93      0.84      0.88      5284\n",
            "           2       0.90      0.62      0.74      7386\n",
            "           3       0.36      0.84      0.51      1425\n",
            "\n",
            "    accuracy                           0.72     14749\n",
            "   macro avg       0.61      0.72      0.61     14749\n",
            "weighted avg       0.83      0.72      0.75     14749\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgCAevMg8t2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}