{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import words_repeated_char\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(file_path, delimiter = ' '):\n",
    "    \"\"\" We had some issues loading the data using pandas.read_csv, so we built our own loader.\n",
    "        Read a csv file, returns a pandas.Dataframe\n",
    "    \"\"\"\n",
    "    fp = open(file_path)\n",
    "    line = fp.readline()\n",
    "    data_dict = dict()\n",
    "    labels = line[:-1].split(delimiter)\n",
    "    line = fp.readline()\n",
    "    for label in labels:\n",
    "        data_dict[label] = []\n",
    "    while line:\n",
    "        for i, j in enumerate(line[:-1].split(delimiter)):\n",
    "            data_dict[labels[i]].append(j)\n",
    "        line = fp.readline()\n",
    "    return pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocess(tweets):\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    X = []\n",
    "    for tweet in tweets:\n",
    "        tmp = tweet.lower().replace(\"\\\\n\",' ').replace('\\\\xa0', ' ').replace('\\\\r', ' ').replace(\"\\\\'\",\"'\").replace(\"&lt;\",'<')\\\n",
    "             .replace(\"&gt;\",'>').replace(\" &amp; \", \" and \").replace(\"&amp;\", \"&\")\n",
    "        # lower the tweet and replacing characters that tweeter has tranlated to their hmtl numeric code to their original value\n",
    "        tmp = re.sub(\"(http(s)?://)?(www\\.)?([a-zA-Z0-9])+\\.[a-z]{1,3}(/\\S*)?\",'URL', tmp) # \n",
    "        tmp = re.sub(\"#\\w+\", 'HASHTAG', tmp)\n",
    "        tmp = re.sub(\"@\\w+\", 'USER', tmp)\n",
    "        tmp = re.sub(\"\\w+@\\w+\\.[a-z]{2,3}\", \"EMAIL\", tmp)\n",
    "        tmp = re.sub(\"[0-9]{1,2}/[0-9]{1,2}/([0-9]{4}|[0-9]{2})|([0-9]{4}|[0-9]{2})/[0-9]{1,2}/[0-9]{1,2}|[0-9]{2}/[0-9]{2}\", \"DATE\", tmp)\n",
    "        tmp = re.sub(\"[0-9]{2}(:[0-9]{2}){1,2}( ?(am|pm))?\", \"TIME\", tmp)\n",
    "        tmp = re.sub(\"(([\\U00002600-\\U000027BF])|([\\U0001f300-\\U0001f64F])|([\\U0001f680-\\U0001f6FF]))+\", ' EMOJI ', tmp)\n",
    "        tmp = tmp.encode(encoding='ascii', errors='ignore').decode()\n",
    "        # get rid of the non ascii characters\n",
    "        tmp = re.sub(\"\\.{2,}\", \"...\", tmp)\n",
    "        for c in ['!', '-', ',']:\n",
    "            tmp = re.sub(\"{}+\".format(c), c, tmp)\n",
    "        tmp = re.sub(\"\\?+\", \"?\", tmp)\n",
    "        tmp = re.sub(\"`+\", \"'\", tmp)\n",
    "        tmp = re.sub(\"'{2,}\", \"'\", tmp)\n",
    "        tmp = tmp.translate(str.maketrans(dict.fromkeys('#*+/<=>@[\\\\]^_`{|}~'))) #removing all the other special characters\n",
    "        tokens = [t if t not in [\"''\", \"``\"] else '\"' for t in word_tokenize(tmp) ]\n",
    "        # tokenizing using nltk.word_tokenize. althought it transforms '\"' into '``' or \"''\" and this is a behaviour do not want\n",
    "        # so we make sure that the '\"' are changed to their original form\n",
    "        for i, token in enumerate(tokens):\n",
    "            if re.search(r\"([a-z])\\1{2,}\", token):\n",
    "                # cleaning the words containing a letter repeated 3 times or more, using the list of the Ensglish words\n",
    "                tokens[i] = words_repeated_char.clean(token)\n",
    "            tokens[i] = lemmatizer.lemmatize(tokens[i])\n",
    "        X.append(tokens)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_csv(\"data/hydrated/hateful_tweets_filtered.csv\", delimiter='\\t')\n",
    "data2 = pd.read_pickle(\"data/originals/labeled_data.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2int = {\"hateful\" : 0, \"abusive\" : 1, \"normal\":2, \"spam\":3}\n",
    "X = preprocess(data[\"tweet_content\"].values)\n",
    "y = [label2int[c] for c in data[\"label\"].values]\n",
    "X2 = preprocess(data2[\"tweet\"].values)\n",
    "X = X + X2\n",
    "y = y + list(data2[\"class\"].values)\n",
    "assert len(X) == len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.DataFrame({\"tweet\" : X, \"label\" : y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3.to_pickle(\"data/preprocess.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
